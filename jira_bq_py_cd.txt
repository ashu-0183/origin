import requests
import json
from google.cloud import bigquery
from datetime import datetime
import pytz

# Jira API credentials
JIRA_BASE_URL = "https://yourdomain.atlassian.net"
JIRA_API_TOKEN = "your_jira_api_token"
JIRA_EMAIL = "your_email@example.com"

# BigQuery configuration
BIGQUERY_DATASET = "your_dataset_id"
BIGQUERY_TABLE = "your_table_id"

# Headers for Jira API
HEADERS = {
    "Authorization": f"Basic {JIRA_EMAIL}:{JIRA_API_TOKEN}",
    "Accept": "application/json"
}

def fetch_jira_data():
    """
    Fetch data from Jira API with pagination
    """
    issues = []
    start_at = 0
    max_results = 100  # Adjust based on performance
    total_issues = None

    while total_issues is None or start_at < total_issues:
        search_url = f"{JIRA_BASE_URL}/rest/api/3/search"
        jql_query = "project=PE AND issuetype in (Feature, Story, Task, Bug)"
        search_params = {
            "jql": jql_query,
            "startAt": start_at,
            "maxResults": max_results,
            "expand": "changelog"
        }
        
        response = requests.get(search_url, headers=HEADERS, params=search_params)
        response.raise_for_status()
        data = response.json()

        if total_issues is None:
            total_issues = data["total"]

        issues.extend(data["issues"])
        start_at += max_results

        # Limiting for testing (Remove this once tested)
        if start_at >= 200:
            break  

    return issues

def transform_data(issues):
    """
    Transform Jira data into BigQuery format
    """
    rows = []

    for issue in issues:
        fields = issue["fields"]

        # Convert timestamps to BigQuery-supported format
        def parse_datetime(value):
            if value:
                dt = datetime.strptime(value, "%Y-%m-%dT%H:%M:%S.%f%z")
                return dt.astimezone(pytz.UTC).isoformat()
            return None

        row = {
            "key": issue["key"],
            "summary": fields.get("summary", ""),
            "status": fields["status"]["name"] if fields.get("status") else None,
            "created": parse_datetime(fields.get("created")),
            "epic_link": fields.get("customfield_10014"),
            "story_points": fields.get("customfield_10028"),
            "assignee": fields["assignee"]["displayName"] if fields.get("assignee") else None,
            "resolution": fields["resolution"]["name"] if fields.get("resolution") else None,
            "resolution_date": parse_datetime(fields.get("resolutiondate")),
            "issue_type": fields["issuetype"]["name"] if fields.get("issuetype") else None,
            "fix_version": [v["name"] for v in fields.get("fixVersions", [])] if fields.get("fixVersions") else None,
            "components": [c["name"] for c in fields.get("components", [])] if fields.get("components") else None,
            "labels": fields.get("labels", []),
            "priority": fields["priority"]["name"] if fields.get("priority") else None,
            "start_date": parse_datetime(fields.get("customfield_10010")),
            "target_end_date": parse_datetime(fields.get("customfield_10011")),
            "due_date": parse_datetime(fields.get("duedate")),
        }

        # Process sprint field (string repeated)
        sprint_data = fields.get("customfield_10020")
        if sprint_data:
            row["sprint"] = [sprint_data] if isinstance(sprint_data, str) else sprint_data
        else:
            row["sprint"] = []

        # Process subtasks
        row["subtask"] = [
            {
                "original_estimate": subtask["fields"].get("timeoriginalestimate"),
                "logged_hours": subtask["fields"].get("timespent")
            }
            for subtask in fields.get("subtasks", [])
        ]

        # Process linked issues
        row["linked_issues"] = [
            {"linked_type": link["type"]["name"]}
            for link in issue.get("fields", {}).get("issuelinks", [])
        ]

        # Process changelog
        row["changelog"] = []
        if "changelog" in issue and "histories" in issue["changelog"]:
            for history in issue["changelog"]["histories"]:
                for item in history["items"]:
                    if item.get("field") == "status":  # Only capturing status changes
                        row["changelog"].append({
                            "history": f"{item.get('fromString', 'None')} â†’ {item.get('toString', 'None')}",
                            "author": history["author"]["displayName"],
                            "timestamp": parse_datetime(history["created"]),
                            "status_change_time": parse_datetime(history["created"]),
                            "field": item.get("field"),
                            "from_status": item.get("fromString"),
                            "to_status": item.get("toString")
                        })

        rows.append(row)

    return rows

def load_to_bigquery(rows):
    """
    Load rows into BigQuery
    """
    client = bigquery.Client()
    table_ref = client.dataset(BIGQUERY_DATASET).table(BIGQUERY_TABLE)

    # Truncate the table before inserting new data
    client.query(f"TRUNCATE TABLE `{BIGQUERY_DATASET}.{BIGQUERY_TABLE}`").result()

    # Insert data in batches to avoid limits
    batch_size = 100
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        errors = client.insert_rows_json(table_ref, batch)
        if errors:
            raise RuntimeError(f"Failed to insert rows: {errors}")

    print("Data successfully inserted into BigQuery!")

def main(request):
    """
    Google Cloud Function entry point
    """
    try:
        # Fetch Jira data
        issues = fetch_jira_data()

        # Transform data
        rows = transform_data(issues)

        # Load data into BigQuery
        load_to_bigquery(rows)

        return "Data pipeline executed successfully!", 200
    except Exception as e:
        return f"Error occurred: {str(e)}", 500
